{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " ASR Model Training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OGZqvWKS8p0w"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a72cfebe896f4caaa48234f770180f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a9af3db689e44164bf2c5af5b938213c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_23c043103e9f49b7a25d06f522539ee1",
              "IPY_MODEL_c806e17b927542ac8d8a452f8e5f9c82"
            ]
          }
        },
        "a9af3db689e44164bf2c5af5b938213c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "23c043103e9f49b7a25d06f522539ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ae02ae6e109341beb386bdc43e28fea6",
            "_dom_classes": [],
            "description": " 58%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 35,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 20.31578947368453,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b24dcca961104852b3b8c4371a3384aa"
          }
        },
        "c806e17b927542ac8d8a452f8e5f9c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f8cf2ea876ed46778bfd146381d21496",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 20.31578947368453/35 [42:26&lt;30:40, 125.36s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d2c5b80ad9fd4f269bfd8708dbde83eb"
          }
        },
        "ae02ae6e109341beb386bdc43e28fea6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b24dcca961104852b3b8c4371a3384aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8cf2ea876ed46778bfd146381d21496": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d2c5b80ad9fd4f269bfd8708dbde83eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshading/Hindi-Facial-Emotion-Synthesis/blob/master/ASR_Model_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU2tucBu-pMN"
      },
      "source": [
        "https://towardsdatascience.com/automatic-speech-recognition-data-collection-with-youtube-v3-api-mask-rcnn-and-google-vision-api-2370d6776109"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sI7SKhvV7Ea",
        "outputId": "208f8271-b067-40bd-fcb2-7edd146edb0d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-8iOF8kd6_h"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Won7JAiPe_wL"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# GPU:\n",
        "!pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# For interactive demo at the end:\n",
        "!pip install pydub\n",
        "# !pip install torchaudio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSiyBLsmrvde",
        "outputId": "a818d3ff-fba2-4bcd-ac18-bb6be8dc6686"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYGPqh-NJpGD"
      },
      "source": [
        "#Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kglRnJLTJqh7"
      },
      "source": [
        "data_dir = '/content/drive/MyDrive/ASR_Project/'\n",
        "au_dir = data_dir+'au_dir/'\n",
        "audio_dir = data_dir+'split_audios_dir/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tH-GqmWHUDSO"
      },
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def load_audios():\n",
        "    onlyfiles = [f for f in listdir(audio_dir) if isfile(join(audio_dir, f))]\n",
        "    audio_dataset = [(f[:-4], *torchaudio.load(audio_dir+f)) for f in onlyfiles]\n",
        "    return audio_dataset\n",
        "\n",
        "def load_aus():\n",
        "    onlyfiles = [f for f in listdir(au_dir) if isfile(join(au_dir,f))]\n",
        "    columns = pd.read_csv(au_dir+onlyfiles[0], delimiter=',').columns[1:]\n",
        "    au_dataset = [(f[:-8], pd.read_csv(au_dir+f, delimiter=',').mean().values[1:]) for f in onlyfiles]\n",
        "    return columns, au_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsJTJ-QBVbCG"
      },
      "source": [
        "audio_dataset = load_audios()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kl5G_U4EUUmA",
        "outputId": "8ec376ba-d119-4719-a8ee-4cb9f0b0726f"
      },
      "source": [
        "print(f\"Total Audio Files: {len(audio_dataset)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Audio Files: 1202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWw8wI9pWukb"
      },
      "source": [
        "au_header, au_dataset = load_aus()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wgFhv0WxW_e7",
        "outputId": "a00a7557-a395-48b7-b358-65af7837285b"
      },
      "source": [
        "print(f\"Total Au Files: {len(au_dataset)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Au Files: 1202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnsyeSAxdXzT",
        "outputId": "15eb45bc-60fc-43e1-f036-333b82c9144a"
      },
      "source": [
        "print(f\"All sample rates: {set([data[2] for data in audio_dataset])}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All sample rates: {44100}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUTe3GEeGuVi"
      },
      "source": [
        "resample_transform = torchaudio.transforms.Resample(44100, 16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "4HRd-V5tT63i",
        "outputId": "e9ff3f83-0cdd-4a69-d89b-ab5b053b88b4"
      },
      "source": [
        "name, waveform, sample_rate = audio_dataset[0]\n",
        "print(f\"Name of file: {name}\")\n",
        "print(f\"Shape of waveform: {resample_transform(waveform).size()}\")\n",
        "print(f\"Sample rate of waveform: {sample_rate}\")\n",
        "\n",
        "plt.plot(waveform.t().numpy());"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name of file: avr_NZZWN6g-235\n",
            "Shape of waveform: torch.Size([2, 81920])\n",
            "Sample rate of waveform: 44100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUO5prBUT_3F"
      },
      "source": [
        "#========== CHECK Speech\n",
        "# display(ipd.Audio(waveform,  rate=sample_rate))\n",
        "# print(f\"Sample Rate: {sample_rate}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aN98sWCYUIux",
        "outputId": "fd6d4253-938b-41eb-adb7-0bb9683c35fc"
      },
      "source": [
        "melspec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000)\n",
        "wave_melspec   = melspec_transform(resample_transform(waveform))\n",
        "print(wave_melspec.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchaudio/functional.py:318: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  \"At least one mel filterbank has all zero values. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 128, 410])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:516: UserWarning: stft will require the return_complex parameter be explicitly  specified in a future PyTorch release. Use return_complex=False  to preserve the current behavior or return_complex=True to return  a complex output. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:653.)\n",
            "  normalized, onesided, return_complex)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:516: UserWarning: The function torch.rfft is deprecated and will be removed in a future PyTorch release. Use the new torch.fft module functions, instead, by importing torch.fft and calling torch.fft.fft or torch.fft.rfft. (Triggered internally at  /pytorch/aten/src/ATen/native/SpectralOps.cpp:590.)\n",
            "  normalized, onesided, return_complex)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCrIpG9FcYt9",
        "outputId": "51d8e40d-f1ef-4851-c7f5-c5c91034cfb9"
      },
      "source": [
        "name, au_array = au_dataset[0]\n",
        "print(f\"AU Header: {au_header}\")\n",
        "print(f\"Name of file: {name}\")\n",
        "print(f\"Shape of AU: {au_array.shape}\")\n",
        "print(f\"AU: {au_array}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AU Header: Index(['AU01_r', 'AU02_r', 'AU04_r', 'AU05_r', 'AU06_r', 'AU07_r', 'AU09_r',\n",
            "       'AU10_r', 'AU12_r', 'AU14_r', 'AU15_r', 'AU17_r', 'AU20_r', 'AU23_r',\n",
            "       'AU25_r', 'AU26_r', 'AU45_r'],\n",
            "      dtype='object')\n",
            "Name of file: avr_NZZWN6g-033\n",
            "Shape of AU: (17,)\n",
            "AU: [0.1425     0.04932692 0.44548077 0.05403846 0.80692308 0.22346154\n",
            " 0.04259615 0.32009615 0.43384615 0.48163462 0.08644231 0.43682692\n",
            " 0.06778846 0.12048077 0.30653846 0.36923077 0.12471154]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyf3_KGbeeX9"
      },
      "source": [
        "audio_mapset = {}\n",
        "for name, wave_arr, _ in audio_dataset:\n",
        "    audio_mapset[name] = wave_arr\n",
        "\n",
        "au_mapset = {}\n",
        "for name, au_arr in au_dataset:\n",
        "    au_mapset[name] = au_arr\n",
        "\n",
        "total_dataset = [(name, resample_transform(audio_mapset[name]), au_mapset[name]) for name in audio_mapset]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSJIlYo_gIab"
      },
      "source": [
        "import random\n",
        "random.shuffle(total_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUoJTAxfHkwK",
        "outputId": "81dc5205-777a-44a2-8f6d-05031d1beb0f"
      },
      "source": [
        "print(f\"Total Length: {len(total_dataset)}\")\n",
        "\n",
        "name, audio_arr, au_arr = total_dataset[0]\n",
        "print(f\"Name: {name}\")\n",
        "print(f\"Normal Audio Shape: {audio_arr.shape}\")\n",
        "print(f\"Mono Audio Shape: {audio_arr.mean(dim=0).shape}\")\n",
        "print(f\"Mono Audo Feature Shape: {melspec_transform(audio_arr.mean(dim=0)).unsqueeze(0).shape}\")\n",
        "print(f\"Au Shape: {au_arr.shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Length: 1202\n",
            "Name: kjZdtb52LRs-026\n",
            "Normal Audio Shape: torch.Size([2, 86100])\n",
            "Mono Audio Shape: torch.Size([86100])\n",
            "Mono Audo Feature Shape: torch.Size([1, 128, 431])\n",
            "Au Shape: (17,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNQFTIgMHrLR",
        "outputId": "80035736-b734-4465-b65d-7b219552a4f1"
      },
      "source": [
        "all_audio_lengths = sorted(set([element[1].shape[1] for element in total_dataset]), reverse=True)\n",
        "max_audio_length = max(all_audio_lengths)\n",
        "print(f\"All Audio Lengths: {all_audio_lengths}\")\n",
        "print(f\"Max Audio Length: {max_audio_length}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Audio Lengths: [88190, 87772, 87354, 86936, 86518, 86100, 85682, 85264, 84846, 84428, 84010, 83174, 82756, 82338, 81920, 81503, 81085, 80667, 80249, 79413, 78995, 77741, 77323, 76905, 76487, 76069, 75651, 74815, 73979, 73561, 72725, 71054, 70636, 70218, 69800, 69382, 68964, 68546, 67292, 66874, 64784, 64366, 63948, 60187, 56425, 56007, 55589, 54335, 52245, 50156, 49738, 49320, 48902, 48484, 47230, 46812, 43886, 43468, 42214, 39289, 38871, 33437, 28840, 28004, 27168, 24660, 24242, 19645, 19227, 14629, 10867, 8352, 7937, 6673, 6256, 5841, 5008, 2080, 1249, 832]\n",
            "Max Audio Length: 88190\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QxmVo0T_iJ9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8fb6d12-dad9-40ee-84f3-9c6c5bf12eab"
      },
      "source": [
        "total_length = len(total_dataset)\n",
        "train_length = int(total_length * 0.9)\n",
        "\n",
        "train_dataset = total_dataset[:train_length]\n",
        "test_dataset = total_dataset[train_length:]\n",
        "print(f\"Train Length: {len(train_dataset)}\")\n",
        "print(f\"Test Length: {len(test_dataset)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Length: 1081\n",
            "Test Length: 121\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxMwNiUIV7eG"
      },
      "source": [
        "#Transformer Based Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0whJ9Oik1kw"
      },
      "source": [
        "Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBiYFAOUbJc"
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # add pe constatns to embeddings\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # print(f\"PE Shape:{self.pe.shape}\")\n",
        "        # print(f\"X shape: {x.shape}\")\n",
        "        # print(f\"Seq Len: {seq_len}\")\n",
        "        # print(f\"Added PE Shape: {self.pe[:,:seq_len,:].shape}\")\n",
        "\n",
        "        x = x + self.pe[:,:seq_len, :]  #.cuda().detach()\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM9RwLVSmB5O"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, N_encoder_layers, heads, d_ff, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.N = N_encoder_layers\n",
        "        # self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, max_seq_len)\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, heads, d_ff, dropout=0.3),\n",
        "            self.N\n",
        "        )\n",
        "        # self.layers = get_clones(\n",
        "        #     EncoderLayer(d_model, heads, d_ff),\n",
        "        #     # nn.TransformerEncoderLayer(d_model, heads, d_ff),\n",
        "        #      self.N)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "    def forward(self, src):\n",
        "        # print(f\"Before PE X shape: {src.shape}\")\n",
        "        x = self.pe(src)\n",
        "\n",
        "        # x = (batch_size, seq_len, d_model)\n",
        "        # but Transformer Encoder layer accepts\n",
        "        # (seq_len, batch_size, d_model)\n",
        "        x = x.transpose(0,1)\n",
        "        x = self.encoder(x)\n",
        "        # for i in range(self.N):\n",
        "        #     x = self.layers[i](x)\n",
        "        x = x.transpose(0,1)\n",
        "        \n",
        "        return self.norm(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTs1PYEQtkGM"
      },
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batcorch.Size([1, 43h norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        # print(f\"Before Layer Norm shape: {x.shape}\")\n",
        "        x = self.layer_norm1(x)\n",
        "        x = torch.tanh(x) #F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = torch.tanh(x) #F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruwZgQ97m8F2"
      },
      "source": [
        "class SpeechModel(nn.Module):\n",
        "    def __init__(self, max_seq_len, n_feats, N_cnn_layers, n_channels,\\\n",
        "                 N_encoder_layers, d_model, d_ff, heads,N_AUs,\\\n",
        "                 stride=2, dropout=0.1,\\\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Conv2d(1, n_channels, 3, stride=1, padding=3//2)\n",
        "        # cnn for extracting heirachal features\n",
        "\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(n_channels, n_channels, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(N_cnn_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(n_channels*n_feats, d_model) #n_channels\n",
        "\n",
        "        self.encoder = Encoder(d_model, N_encoder_layers, heads, d_ff, max_seq_len)\n",
        "        self.linear = nn.Linear(d_model, N_AUs)\n",
        "\n",
        "    def forward(self, src):  # input: (batch, 1, feature, time)\n",
        "        x = self.cnn(src)    # (batch, channel, feature, time)\n",
        "        # print(f\"After CNN shape: {x}\")\n",
        "\n",
        "        x = self.rescnn_layers(x)\n",
        "        # print(f\"After RES shape: {x}\")\n",
        "\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1]*sizes[2], sizes[3])  # (batch, channel*feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, channel*feature)\n",
        "        x = self.fc(x)        # (batch, time, d_model)\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        # print(f\"Befor Encoder X shape: {x}\")\n",
        "\n",
        "        e_outputs = self.encoder(x) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # print(f\"Encoder Output: {e_outputs}\")\n",
        "        avg_enc   = torch.mean(x, -2)   # (batch_size, d_model)\n",
        "\n",
        "        # print(f\"Average Encoder: {avg_enc}\")\n",
        "        output = self.linear(avg_enc)  #(batch_size, N_AUs)\n",
        "\n",
        "        # print(f\"Output before Sigmoid: {output}\")\n",
        "        output = 5*nn.Sigmoid()(output)\n",
        "        # print(f\"Output : {output}\")\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bC-QBkQY07An"
      },
      "source": [
        "#Data Loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aX8hGLF08Nq"
      },
      "source": [
        "def pad_sequence(batch):\n",
        "    # Make all tensor in a batch the same length by padding with zeros\n",
        "    batch = [item.t() for item in batch]\n",
        "    batch = torch.nn.utils.rnn.pad_sequence(batch, batch_first=True, padding_value=0.)\n",
        "\n",
        "      # >>> a = torch.ones(25, 300)\n",
        "      # >>> b = torch.ones(22, 300)\n",
        "      # >>> c = torch.ones(15, 300)\n",
        "      # >>> pad_sequence([a, b, c], True).size() = torch.Size([3, 25, 300])\n",
        "      # Pad Mask Not Requires since spectogram take care of zeros\n",
        "    return batch.permute(0, 2, 1)\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "\n",
        "    # A data tuple has the form:\n",
        "    # name, audo_arr, au_arr\n",
        "\n",
        "    tensors, targets = [], []\n",
        "\n",
        "    # Gather in lists, and encode labels as indices\n",
        "    for name, audio_arr, au_arr in batch:\n",
        "        # print(f\"Audio Arr: {audio_arr.shape}\")\n",
        "        # print(f\"Au Arr: {au_arr.shape}\")\n",
        "        tensors += [audio_arr.mean(dim=0, keepdims=True)]\n",
        "        targets += [torch.tensor(au_arr)]\n",
        "\n",
        "    # Group the list of tensors into a batched tensor\n",
        "    tensors = pad_sequence(tensors)\n",
        "    targets = torch.stack(targets)\n",
        "    # print(f\"Tensors Shape: {tensors.shape}\")\n",
        "    # print(f\"Target Shape: {targets.shape}\")\n",
        "\n",
        "    return tensors, targets\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "# Train Length = 1081, Total Batches = 1081/32 = 34\n",
        "\n",
        "if device == \"cuda\":\n",
        "    num_workers = 1\n",
        "    pin_memory = True\n",
        "else:\n",
        "    num_workers = 0\n",
        "    pin_memory = False\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    collate_fn=collate_fn,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=pin_memory,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUcpUSgKk2oV",
        "outputId": "17a41327-4c5d-4d84-be8f-030ecf5d57e6"
      },
      "source": [
        "max_seq_length = 441 # We already know\n",
        "\n",
        "is_find_max_seq = False\n",
        "\n",
        "if is_find_max_seq:\n",
        "    for data, au in train_loader:\n",
        "        print(melspec_transform(data.to(device)).shape)\n",
        "    for data, au in test_loader:\n",
        "        print(melspec_transform(data.to(device)).shape)\n",
        "\n",
        "print(f\"Max Audio Length: {max_audio_length}\")\n",
        "print(f\"Max Seq Length: {max_seq_length}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max Audio Length: 88190\n",
            "Max Seq Length: 441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRE2hTGjw6UG"
      },
      "source": [
        "#Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkYFNJoLrtfi"
      },
      "source": [
        "heads = 8\n",
        "N_encoder_layers = 4\n",
        "d_model = 512 #512\n",
        "d_ff = 2048 #2048\n",
        "N_AUs = 17  # TO BE EDIT\n",
        "\n",
        "max_seq_len = 441 #max_seq_length # TO BE EDIT\n",
        "n_channels = 16\n",
        "N_cnn_layers = 1\n",
        "n_feats = 128 # TO BE EDIT\n",
        "n_mels = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_75ORMExjSL"
      },
      "source": [
        "model = SpeechModel(max_seq_len, n_feats, N_cnn_layers, n_channels, N_encoder_layers,\\\n",
        "                    d_model, d_ff, heads, N_AUs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFk_1Ely6BRn"
      },
      "source": [
        "# Very Important to initialize parameters such that it stops\n",
        "# signal from fading or getting too big\n",
        "for p in model.parameters():\n",
        "  if p.dim() > 1:\n",
        "    nn.init.xavier_uniform_(p)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4uPsNknCF9a"
      },
      "source": [
        "clip_value = 1.0\n",
        "for p in model.parameters():\n",
        "  p.register_hook(lambda grad: torch.clamp(grad, -clip_value, clip_value))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV_mUkFk69F6",
        "outputId": "cadc9f95-903a-423a-d364-6635c0735abc"
      },
      "source": [
        "print(f\"No of parameters: {sum([p.numel() for p in model.parameters() if p.requires_grad])}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of parameters: 13673681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-cscts17ZDp",
        "outputId": "852e19a5-e008-42e3-fecd-8e5893278bf9"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SpeechModel(\n",
              "  (cnn): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (rescnn_layers): Sequential(\n",
              "    (0): ResidualCNN(\n",
              "      (cnn1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (cnn2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (dropout1): Dropout(p=0.1, inplace=False)\n",
              "      (dropout2): Dropout(p=0.1, inplace=False)\n",
              "      (layer_norm1): CNNLayerNorm(\n",
              "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (layer_norm2): CNNLayerNorm(\n",
              "        (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=2048, out_features=512, bias=True)\n",
              "  (encoder): Encoder(\n",
              "    (pe): PositionalEncoder(\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.3, inplace=False)\n",
              "          (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (1): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.3, inplace=False)\n",
              "          (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (2): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.3, inplace=False)\n",
              "          (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "        (3): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (dropout): Dropout(p=0.3, inplace=False)\n",
              "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.3, inplace=False)\n",
              "          (dropout2): Dropout(p=0.3, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (linear): Linear(in_features=512, out_features=17, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqsmHC217SnR"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axyxehlz2KY1"
      },
      "source": [
        "def train(model, epoch, log_interval):\n",
        "    model.to(device).double()\n",
        "    melspec_transform.to(device)\n",
        "\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "\n",
        "        data = melspec_transform(data.to(device)).to(device) # (batch_size, 1, feature, time)\n",
        "        target = target.to(device).double()\n",
        "        target[torch.isnan(target)] = 0\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # print(f\"Before CNN: {data}\")\n",
        "        output = model(data.double())  #(batch_size, N_AUs)\n",
        "    \n",
        "        output =  output.view(-1, 1)\n",
        "        target =  target.view(-1, 1)\n",
        "        loss = F.mse_loss(output, target).double() # Mse Loss On(batch x N_AUs)\n",
        "\n",
        "        l2_lambda = 0.1  #0.01\n",
        "        l2_reg = torch.tensor(0.).to(device)\n",
        "        for param in model.parameters():\n",
        "          l2_reg += torch.norm(param)\n",
        "        loss += l2_lambda * l2_reg\n",
        "        # print(f\"Data Type: {data.dtype}\")\n",
        "        # print(f\"Output Type: {output.dtype}\")\n",
        "        # print(f\"Loss Type: {loss.dtype}\")\n",
        "\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        # print training stats\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
        "\n",
        "        # update progress bar\n",
        "        pbar.update(pbar_update)\n",
        "        # record loss\n",
        "        losses.append(loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFS_buEpLWdJ"
      },
      "source": [
        "def test(model, epoch):\n",
        "    model.to(device).double()\n",
        "    melspec_transform.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for data, target in test_loader:\n",
        "\n",
        "          data = melspec_transform(data.to(device)).to(device) # (batch_size, 1, feature, time)\n",
        "          target = target.to(device).double()\n",
        "          target[torch.isnan(target)] = 0\n",
        "\n",
        "          output = model(data.double())  #(batch_size, N_AUs)\n",
        "\n",
        "          # pred = get_likely_index(output)\n",
        "          # correct += number_of_correct(pred, target)\n",
        "          # print(f\"output:{output.shape}\")\n",
        "          # print(f\"target: {target.shape}\")\n",
        "          # print(f\"Data Type: {data.dtype}\")\n",
        "          # print(f\"Output Type: {output.dtype}\")\n",
        "\n",
        "          loss += F.mse_loss(output, target).double()\n",
        "          # print(loss)\n",
        "\n",
        "          # update progress bar\n",
        "          # pbar.update(pbar_update)\n",
        "\n",
        "    loss = loss/len(test_loader.dataset)\n",
        "    print(f\"\\n Test Epoch: {epoch} \\t Loss: {loss:.4f} \\n\")\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuWvHn8NMyXD",
        "outputId": "6787e27e-de10-4e58-e327-b6bdd70d661f"
      },
      "source": [
        "test(model, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Test Epoch: 1 \t Loss: 0.1733 \n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1733, device='cuda:0', dtype=torch.float64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4S21S1ed6loy"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.1)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=35, gamma=0.1)  # reduce the learning after 20 epochs by a factor of 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_JwwtVXHS_h"
      },
      "source": [
        "for i,(name, child) in enumerate(model.named_children()):\n",
        "  for param in child.parameters():\n",
        "    print(f\"name: {name} => param: {param}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0K7r5JFP3h93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a72cfebe896f4caaa48234f770180f29",
            "a9af3db689e44164bf2c5af5b938213c",
            "23c043103e9f49b7a25d06f522539ee1",
            "c806e17b927542ac8d8a452f8e5f9c82",
            "ae02ae6e109341beb386bdc43e28fea6",
            "b24dcca961104852b3b8c4371a3384aa",
            "f8cf2ea876ed46778bfd146381d21496",
            "d2c5b80ad9fd4f269bfd8708dbde83eb"
          ]
        },
        "outputId": "68bb61c7-dfa4-44d2-9661-0ed438371856"
      },
      "source": [
        "log_interval = 20\n",
        "#============ UPDATING number of epochs ====================\n",
        "n_epoch = 35\n",
        "\n",
        "pbar_update = 1 / (len(train_loader)+len(test_loader))\n",
        "losses = []\n",
        "\n",
        "# The transform needs to live on the same device as the model and the data.\n",
        "melspec_transform = melspec_transform.to(device)\n",
        "model = model.to(device)\n",
        "with tqdm(total=n_epoch) as pbar:\n",
        "    for epoch in range(1, n_epoch + 1):\n",
        "        train(model, epoch, log_interval)\n",
        "        losses += [test(model, epoch)]\n",
        "        if epoch % 7 == 0:\n",
        "          torch.save(model.state_dict(), data_dir+'/SunilModel')\n",
        "        scheduler.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a72cfebe896f4caaa48234f770180f29",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=35.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/1081 (0%)]\tLoss: 76.071918\n",
            "Train Epoch: 1 [640/1081 (59%)]\tLoss: 69.737045\n",
            "\n",
            " Test Epoch: 1 \t Loss: 0.0243 \n",
            "\n",
            "Train Epoch: 2 [0/1081 (0%)]\tLoss: 67.123883\n",
            "Train Epoch: 2 [640/1081 (59%)]\tLoss: 64.042819\n",
            "\n",
            " Test Epoch: 2 \t Loss: 0.0062 \n",
            "\n",
            "Train Epoch: 3 [0/1081 (0%)]\tLoss: 62.264362\n",
            "Train Epoch: 3 [640/1081 (59%)]\tLoss: 59.956676\n",
            "\n",
            " Test Epoch: 3 \t Loss: 0.0058 \n",
            "\n",
            "Train Epoch: 4 [0/1081 (0%)]\tLoss: 58.379173\n",
            "Train Epoch: 4 [640/1081 (59%)]\tLoss: 56.169008\n",
            "\n",
            " Test Epoch: 4 \t Loss: 0.0056 \n",
            "\n",
            "Train Epoch: 5 [0/1081 (0%)]\tLoss: 54.773908\n",
            "Train Epoch: 5 [640/1081 (59%)]\tLoss: 52.821775\n",
            "\n",
            " Test Epoch: 5 \t Loss: 0.0055 \n",
            "\n",
            "Train Epoch: 6 [0/1081 (0%)]\tLoss: 51.513827\n",
            "Train Epoch: 6 [640/1081 (59%)]\tLoss: 49.688415\n",
            "\n",
            " Test Epoch: 6 \t Loss: 0.0053 \n",
            "\n",
            "Train Epoch: 7 [0/1081 (0%)]\tLoss: 48.494338\n",
            "Train Epoch: 7 [640/1081 (59%)]\tLoss: 46.842409\n",
            "\n",
            " Test Epoch: 7 \t Loss: 0.0054 \n",
            "\n",
            "Train Epoch: 8 [0/1081 (0%)]\tLoss: 45.725490\n",
            "Train Epoch: 8 [640/1081 (59%)]\tLoss: 44.228421\n",
            "\n",
            " Test Epoch: 8 \t Loss: 0.0053 \n",
            "\n",
            "Train Epoch: 9 [0/1081 (0%)]\tLoss: 43.191196\n",
            "Train Epoch: 9 [640/1081 (59%)]\tLoss: 41.844079\n",
            "\n",
            " Test Epoch: 9 \t Loss: 0.0051 \n",
            "\n",
            "Train Epoch: 10 [0/1081 (0%)]\tLoss: 40.835008\n",
            "Train Epoch: 10 [640/1081 (59%)]\tLoss: 39.606544\n",
            "\n",
            " Test Epoch: 10 \t Loss: 0.0052 \n",
            "\n",
            "Train Epoch: 11 [0/1081 (0%)]\tLoss: 38.726701\n",
            "Train Epoch: 11 [640/1081 (59%)]\tLoss: 37.564185\n",
            "\n",
            " Test Epoch: 11 \t Loss: 0.0051 \n",
            "\n",
            "Train Epoch: 12 [0/1081 (0%)]\tLoss: 36.762227\n",
            "Train Epoch: 12 [640/1081 (59%)]\tLoss: 35.708031\n",
            "\n",
            " Test Epoch: 12 \t Loss: 0.0051 \n",
            "\n",
            "Train Epoch: 13 [0/1081 (0%)]\tLoss: 35.010911\n",
            "Train Epoch: 13 [640/1081 (59%)]\tLoss: 33.998937\n",
            "\n",
            " Test Epoch: 13 \t Loss: 0.0050 \n",
            "\n",
            "Train Epoch: 14 [0/1081 (0%)]\tLoss: 33.389815\n",
            "Train Epoch: 14 [640/1081 (59%)]\tLoss: 32.446731\n",
            "\n",
            " Test Epoch: 14 \t Loss: 0.0052 \n",
            "\n",
            "Train Epoch: 15 [0/1081 (0%)]\tLoss: 31.895263\n",
            "Train Epoch: 15 [640/1081 (59%)]\tLoss: 31.073480\n",
            "\n",
            " Test Epoch: 15 \t Loss: 0.0051 \n",
            "\n",
            "Train Epoch: 16 [0/1081 (0%)]\tLoss: 30.519118\n",
            "Train Epoch: 16 [640/1081 (59%)]\tLoss: 29.820998\n",
            "\n",
            " Test Epoch: 16 \t Loss: 0.0051 \n",
            "\n",
            "Train Epoch: 17 [0/1081 (0%)]\tLoss: 29.342432\n",
            "Train Epoch: 17 [640/1081 (59%)]\tLoss: 28.669270\n",
            "\n",
            " Test Epoch: 17 \t Loss: 0.0051 \n",
            "\n",
            "Train Epoch: 18 [0/1081 (0%)]\tLoss: 28.266277\n",
            "Train Epoch: 18 [640/1081 (59%)]\tLoss: 27.637969\n",
            "\n",
            " Test Epoch: 18 \t Loss: 0.0051 \n",
            "\n",
            "Train Epoch: 19 [0/1081 (0%)]\tLoss: 27.248983\n",
            "Train Epoch: 19 [640/1081 (59%)]\tLoss: 26.683247\n",
            "\n",
            " Test Epoch: 19 \t Loss: 0.0053 \n",
            "\n",
            "Train Epoch: 20 [0/1081 (0%)]\tLoss: 26.352488\n",
            "Train Epoch: 20 [640/1081 (59%)]\tLoss: 25.836924\n",
            "\n",
            " Test Epoch: 20 \t Loss: 0.0053 \n",
            "\n",
            "Train Epoch: 21 [0/1081 (0%)]\tLoss: 25.542227\n",
            "Train Epoch: 21 [640/1081 (59%)]\tLoss: 25.096689\n",
            "\n",
            " Test Epoch: 21 \t Loss: 0.0054 \n",
            "\n",
            "Train Epoch: 22 [0/1081 (0%)]\tLoss: 24.789543\n",
            "Train Epoch: 22 [640/1081 (59%)]\tLoss: 24.391090\n",
            "\n",
            " Test Epoch: 22 \t Loss: 0.0055 \n",
            "\n",
            "Train Epoch: 23 [0/1081 (0%)]\tLoss: 24.099390\n",
            "Train Epoch: 23 [640/1081 (59%)]\tLoss: 23.758418\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-107-e14b85d0b6a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m7\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-102-6610e9d188f2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch, log_interval)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# print(f\"Before CNN: {data}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdouble\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#(batch_size, N_AUs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-8b32a8243d04>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# print(f\"Befor Encoder X shape: {x}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0me_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch_size, seq_len, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# print(f\"Encoder Output: {e_outputs}\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-c1130ff91bee>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# (seq_len, batch_size, d_model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;31m# for i in range(self.N):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m#     x = self.layers[i](x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \"\"\"\n\u001b[1;32m    293\u001b[0m         src2 = self.self_attn(src, src, src, attn_mask=src_mask,\n\u001b[0;32m--> 294\u001b[0;31m                               key_padding_mask=src_key_padding_mask)[0]\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    983\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4144\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4145\u001b[0m             \u001b[0;31m# self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4146\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLoqrQ60tLmJ"
      },
      "source": [
        "# Show Stored Results (Running the model gives following train and test losses)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cnVJtXxtNp1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs       = list(range(1, 11))\n",
        "test_losses  = [0.0243, 0.0062, 0.0058, 0.0056, 0.0055, 0.0053, 0.0054, 0.0053, 0.0051, 0.0052]\n",
        "train_losses = [76.071918, 67.123883, 62.264362, 58.379173, 54.773908, \n",
        "                51.513827, 48.494338, 45.725490, 43.191196, 40.835008]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQDObByrtcxW"
      },
      "source": [
        "plt.figure(figsize=(8, 6), dpi=80)\n",
        "plt.plot(epochs, train_losses, marker='o', color='r')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Train Mse Loss with L2 Regularization\")\n",
        "plt.title(\"Train loss va epochs\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rjJI8VYtnRq"
      },
      "source": [
        "plt.figure(figsize=(8, 6), dpi=80)\n",
        "plt.plot(epochs, test_losses, marker='o', color='r')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Test Mse Loss\")\n",
        "plt.title(\"Test loss va epochs\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Master.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshading/Hindi-Facial-Emotion-Synthesis/blob/master/Master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUM7XNF0WXeg",
        "outputId": "488950da-8148-4943-95a3-c320ea94d628"
      },
      "source": [
        "!git clone https://github.com/harshading/Hindi-Facial-Emotion-Synthesis.git\n",
        "!cd Hindi-Facial-Emotion-Synthesis\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Hindi-Facial-Emotion-Synthesis'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Counting objects: 100% (73/73), done.\u001b[K\n",
            "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
            "remote: Total 73 (delta 15), reused 47 (delta 2), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mC1U5Aup13x"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# GPU:\n",
        "# !pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# For interactive demo at the end:\n",
        "!pip install pydub\n",
        "!pip install torchaudio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryCeUq3yvyS3"
      },
      "source": [
        "import os, subprocess\n",
        "import csv\n",
        "import pickle"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3hORWpep9F1",
        "outputId": "50f3d144-8b5b-491a-8560-fbda24e322f4"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPxUz8_2WbJj",
        "outputId": "5f1fe084-f456-4c7e-cab8-34aea3f17a11"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UtXl6sWqDm3"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PCjkofqqKvC"
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # add pe constatns to embeddings\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # print(f\"PE Shape:{self.pe.shape}\")\n",
        "        # print(f\"X shape: {x.shape}\")\n",
        "        # print(f\"Seq Len: {seq_len}\")\n",
        "        # print(f\"Added PE Shape: {self.pe[:,:seq_len,:].shape}\")\n",
        "\n",
        "        x = x + self.pe[:,:seq_len, :]  #.cuda().detach()\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, N_encoder_layers, heads, d_ff, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.N = N_encoder_layers\n",
        "        # self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, max_seq_len)\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, heads, d_ff, dropout=0.3),\n",
        "            self.N\n",
        "        )\n",
        "        # self.layers = get_clones(\n",
        "        #     EncoderLayer(d_model, heads, d_ff),\n",
        "        #     # nn.TransformerEncoderLayer(d_model, heads, d_ff),\n",
        "        #      self.N)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "    def forward(self, src):\n",
        "        # print(f\"Before PE X shape: {src.shape}\")\n",
        "        x = self.pe(src)\n",
        "\n",
        "        # x = (batch_size, seq_len, d_model)\n",
        "        # but Transformer Encoder layer accepts\n",
        "        # (seq_len, batch_size, d_model)\n",
        "        x = x.transpose(0,1)\n",
        "        x = self.encoder(x)\n",
        "        # for i in range(self.N):\n",
        "        #     x = self.layers[i](x)\n",
        "        x = x.transpose(0,1)\n",
        "        \n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batcorch.Size([1, 43h norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        # print(f\"Before Layer Norm shape: {x.shape}\")\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "class SpeechModel(nn.Module):\n",
        "    def __init__(self, max_seq_len, n_feats, N_cnn_layers, n_channels,\\\n",
        "                 N_encoder_layers, d_model, d_ff, heads,N_AUs,\\\n",
        "                 stride=2, dropout=0.1,\\\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Conv2d(1, n_channels, 3, stride=1, padding=3//2)\n",
        "        # cnn for extracting heirachal features\n",
        "\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(n_channels, n_channels, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(N_cnn_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(n_channels*n_feats, d_model) #n_channels\n",
        "\n",
        "        self.encoder = Encoder(d_model, N_encoder_layers, heads, d_ff, max_seq_len)\n",
        "        self.linear = nn.Linear(d_model, N_AUs)\n",
        "\n",
        "    def forward(self, src):  # input: (batch, 1, feature, time)\n",
        "        x = self.cnn(src)    # (batch, channel, feature, time)\n",
        "        # print(f\"After CNN shape: {x}\")\n",
        "\n",
        "        x = self.rescnn_layers(x)\n",
        "        # print(f\"After RES shape: {x}\")\n",
        "\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1]*sizes[2], sizes[3])  # (batch, channel*feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, channel*feature)\n",
        "        x = self.fc(x)        # (batch, time, d_model)\n",
        "\n",
        "        # print(f\"Befor Encoder X shape: {x}\")\n",
        "\n",
        "        e_outputs = self.encoder(x) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # print(f\"Encoder Output: {e_outputs}\")\n",
        "        avg_enc   = torch.mean(x, -2)   # (batch_size, d_model)\n",
        "\n",
        "        # print(f\"Average Encoder: {avg_enc}\")\n",
        "        output = self.linear(avg_enc)  #(batch_size, N_AUs)\n",
        "\n",
        "        # print(f\"Output before Sigmoid: {output}\")\n",
        "        output = 5*nn.Sigmoid()(output)\n",
        "        # print(f\"Output : {output}\")\n",
        "        return output"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz8_aQmAqOSh"
      },
      "source": [
        "heads = 8\n",
        "N_encoder_layers = 4\n",
        "d_model = 512 #512\n",
        "d_ff = 2048 #2048\n",
        "N_AUs = 17  # TO BE EDIT\n",
        "\n",
        "max_seq_len = 441 #max_seq_length # TO BE EDIT\n",
        "n_channels = 16\n",
        "N_cnn_layers = 1\n",
        "n_feats = 128 # TO BE EDIT\n",
        "n_mels = 128\n",
        "\n",
        "model = SpeechModel(max_seq_len, n_feats, N_cnn_layers, n_channels, N_encoder_layers,\\\n",
        "                    d_model, d_ff, heads, N_AUs)\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/ASR_Project/'\n",
        "model.load_state_dict(torch.load(data_dir+'TrainedTransformerModel', map_location=torch.device('cpu')))\n",
        "\n",
        "au_dir = data_dir+'au_dir/'\n",
        "audio_dir = data_dir+'split_audios_dir/'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujLrIN7Uqklb",
        "outputId": "0e5715ab-9524-4673-b1f5-dd70c146a90f"
      },
      "source": [
        "import pandas as pd\n",
        "resample_transform = torchaudio.transforms.Resample(44100, 16000)\n",
        "melspec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000)\n",
        "\n",
        "\n",
        "def get_audio(file_path):\n",
        "    waveform, _ = torchaudio.load(file_path)\n",
        "    waveform = resample_transform(waveform)\n",
        "    return waveform\n",
        "\n",
        "def get_au(file_path):\n",
        "    df = pd.read_csv(file_path, delimiter=',')\n",
        "    columns = df.columns[1:]\n",
        "    au_data = torch.tensor(df.mean().values[1:])\n",
        "    return au_data, columns\n",
        "\n",
        "\n",
        "def get_output(model, wave):\n",
        "    model.eval()\n",
        "    wave_melspec = melspec_transform(wave)\n",
        "    wave_data    = wave_melspec.mean(dim=0, keepdims=True).unsqueeze(0)\n",
        "    model_output = model(wave_data)[0]\n",
        "    return model_output\n",
        "\n",
        "\n",
        "def inference(model, audio_file_path):\n",
        "    waveform = get_audio(audio_file_path)\n",
        "    # target_aus, au_header = get_au(au_file_path)\n",
        "\n",
        "    generated_aus = get_output(model, waveform)\n",
        "\n",
        "    # loss = nn.\n",
        "    return generated_aus"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchaudio/functional/functional.py:358: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  \"At least one mel filterbank has all zero values. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaNxxJr8sroZ"
      },
      "source": [
        "# Upload your audio clip (Input A) and Face Image (Input B)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml0RavWvqnqK"
      },
      "source": [
        "my_audio_file_path = '/content/7qgWBZu9VOc-008.mp3' # this should be replace by path/to/audio/file\n",
        "my_img_file_path = '/content/sized2_2.jpg'\n",
        "\n",
        "base_img_filename = os.path.basename(my_img_file_path)\n",
        "raw_img_filename = os.path.splitext(base_img_filename)[0]\n",
        "# print(base_img_filename)\n",
        "# print(raw_img_filename)\n",
        "# my_au_file_path = '/content/W9nIIu9f-To-000.mp4.csv'\n"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LrRtukdwRfI"
      },
      "source": [
        "generated_au = inference(model, my_audio_file_path)\n",
        "gen_au = generated_au.detach().numpy()"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UWIftjhSwSDT",
        "outputId": "896561b5-3dd9-4cc2-dfb4-449f856fb1b1"
      },
      "source": [
        "cmd = f'mkdir /content/memotionNet/'.split(\" \")\n",
        "subprocess.run(cmd)\n",
        "\n",
        "cmd1 = f'mkdir /content/memotionNet/imgs/'.split(\" \")\n",
        "subprocess.run(cmd1)\n",
        "\n",
        "cmd2 = f'cp {my_img_file_path} /content/memotionNet/imgs/'.split(\" \")\n",
        "subprocess.run(cmd2)"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CompletedProcess(args=['cp', '/content/sized2_2.jpg', '/content/memotionNet/imgs/'], returncode=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQCXFWQaq8OR"
      },
      "source": [
        "data_dic = {\n",
        "    raw_img_filename: gen_au\n",
        "}\n",
        "\n",
        "filename = '/content/aus_openface.pkl'\n",
        "outfile = open(filename,'wb')\n",
        "pickle.dump(data_dic, outfile)\n",
        "outfile.close()\n",
        "\n",
        "!mv /content/aus_openface.pkl /content/memotionNet/"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RpXxtlG0ox5"
      },
      "source": [
        "!touch /content/test_ids.csv\n",
        "\n",
        "with open('/content/test_ids.csv', mode='w') as f:\n",
        "    f_writer = csv.writer(f, delimiter=',')\n",
        "\n",
        "    f_writer.writerow([base_img_filename])\n",
        "\n",
        "! cp /content/test_ids.csv /content/memotionNet"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qw3XTXUGXZn3"
      },
      "source": [
        "root_dir = '/content/drive/MyDrive/ASR_Project/'\n",
        "audios_dir = 'audios_dir/'\n",
        "processed_dir = '/content/drive/MyDrive/ASR_Project/processed_dir/'\n",
        "frames_dir = '/content/drive/MyDrive/ASR_Project/frames_dir/'\n",
        "split_videos_dir = '/content/drive/MyDrive/ASR_Project/split_videos_dir/'\n",
        "split_audios_dir = '/content/drive/MyDrive/ASR_Project/split_audios_dir/'\n",
        "au_dir = '/content/drive/MyDrive/ASR_Project/au_dir/'\n",
        "\n",
        "GANimation_dir = '/content/drive/MyDrive/ASR_Project/GANimation/'\n",
        "ckpts_dir = '/content/drive/MyDrive/ASR_Project/GANimation/ckpts/'\n",
        "datasets_dir = '/content/drive/MyDrive/ASR_Project/GANimation/emotion/'\n",
        "pretrainedemo_dir = '/content/drive/MyDrive/ASR_Project/GANimation/PretrainedEmo/'"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHl4gBWKXjdt",
        "outputId": "473159b1-149f-4922-9489-34ffdd459228"
      },
      "source": [
        "# cmd = f'python3 ganimation_replicate/main.py --mode test --data_root {GANimation_dir}/my_dataset/ --batch_size 1 --ckpt_dir {ckpts_dir}/emotionNet/ganimation/190327_161852/ --load_epoch 30 --save_test_gif'\n",
        "\n",
        "!cd Hindi-Facial-Emotion-Synthesis && python3 main.py --mode test --data_root /content/memotionNet/ --batch_size 1 --ckpt_dir /content/drive/MyDrive/ASR_Project/GANimation/mckpts/emotionNet/ganimation/190327_160828/ --load_epoch 30 --save_test_gif\n",
        "\n",
        "\n",
        "# import subprocess\n",
        "\n",
        "# subprocess.run(cmd)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------- [ test][210514_181029]Options --------------------\n",
            "                   aus_nc: 17                            \n",
            "                  aus_pkl: aus_openface.pkl              \n",
            "               batch_size: 1                             \t[default: 25]\n",
            "                    beta1: 0.5                           \n",
            "                 ckpt_dir: /content/drive/MyDrive/ASR_Project/GANimation/mckpts/emotionNet/ganimation/190327_160828/\t[default: ./ckpts]\n",
            "                data_root: /content/memotionNet/         \t[default: None]\n",
            "              epoch_count: 1                             \n",
            "               final_size: 128                           \n",
            "                 gan_type: wgan-gp                       \n",
            "                  gpu_ids: [0]                           \t[default: 0]\n",
            "                   img_nc: 3                             \n",
            "                 imgs_dir: imgs                          \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "          interpolate_len: 5                             \n",
            "               lambda_aus: 160.0                         \n",
            "               lambda_dis: 1.0                           \n",
            "              lambda_mask: 0                             \n",
            "               lambda_rec: 10.0                          \n",
            "                lambda_tv: 0                             \n",
            "           lambda_wgan_gp: 10.0                          \n",
            "               load_epoch: 30                            \t[default: 0]\n",
            "                load_size: 148                           \n",
            "                 log_file: logs.txt                      \n",
            "                       lr: 0.0001                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: lambda                        \n",
            "               lucky_seed: 1621015829                    \t[default: 0]\n",
            "         max_dataset_size: inf                           \n",
            "                     mode: test                          \t[default: train]\n",
            "                    model: ganimation                    \n",
            "                n_threads: 6                             \n",
            "                     name: 210514_181029                 \n",
            "                      ndf: 64                            \n",
            "                      ngf: 64                            \n",
            "                    niter: 20                            \n",
            "              niter_decay: 10                            \n",
            "             no_aus_noise: False                         \n",
            "                  no_flip: False                         \n",
            "             no_test_eval: False                         \n",
            "                     norm: instance                      \n",
            "                 opt_file: opt.txt                       \n",
            "         plot_losses_freq: 20000                         \n",
            "        print_losses_freq: 100                           \n",
            "           resize_or_crop: none                          \n",
            "                  results: results/memotionNet_ganimation_30\t[default: results]\n",
            "          sample_img_freq: 2000                          \n",
            "          save_epoch_freq: 2                             \n",
            "            save_test_gif: True                          \t[default: False]\n",
            "           serial_batches: False                         \n",
            "                 test_csv: test_ids.csv                  \n",
            "                train_csv: train_ids.csv                 \n",
            "           train_gen_iter: 5                             \n",
            "              use_dropout: False                         \n",
            "        visdom_display_id: 0                             \t[default: 1]\n",
            "               visdom_env: main                          \n",
            "              visdom_port: 8097                          \n",
            "--------------------- [ test][210514_181029]End ----------------------\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "initialize network with normal\n",
            "[Info] Successfully load trained weights for net_gen.\n",
            "Test with Model [GANimation]\n",
            "Set model to Test state.\n",
            "Set net_gen to EVAL.\n",
            "38\n",
            "[0.7896003  0.6537746  1.1176057  0.64682376 0.9194159  1.3153814\n",
            " 0.66935724 1.1891489  0.919821   1.0739055  0.73496497 0.98586625\n",
            " 0.6794081  0.6868584  1.1004884  1.0184083  0.78645414]\n",
            "38\n",
            "[0.7896003  0.6537746  1.1176057  0.64682376 0.9194159  1.3153814\n",
            " 0.66935724 1.1891489  0.919821   1.0739055  0.73496497 0.98586625\n",
            " 0.6794081  0.6868584  1.1004884  1.0184083  0.78645414]\n",
            "[Success] Saved images to results/memotionNet_ganimation_30/sized2_2_sized2_2.gif\n",
            "[THE END]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/Hindi-Facial-Emotion-Synthesis/results/memotionNet_ganimation_30/sized2_2_sized2_2.gif": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "lhXPox7ZEwE7",
        "outputId": "b3ffb015-d7c3-4c8c-c362-6c2521f071f2"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url='Hindi-Facial-Emotion-Synthesis/results/memotionNet_ganimation_30/sized2_2_sized2_2.gif')"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"Hindi-Facial-Emotion-Synthesis/results/memotionNet_ganimation_30/sized2_2_sized2_2.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " ASR MODEL INFERENCE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OGZqvWKS8p0w"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshading/Hindi-Facial-Emotion-Synthesis/blob/master/ASR_MODEL_INFERENCE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU2tucBu-pMN"
      },
      "source": [
        "https://towardsdatascience.com/automatic-speech-recognition-data-collection-with-youtube-v3-api-mask-rcnn-and-google-vision-api-2370d6776109"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sI7SKhvV7Ea",
        "outputId": "b6d24427-41e3-473c-fc0c-a4155dfbcd14"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-8iOF8kd6_h"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Won7JAiPe_wL"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# GPU:\n",
        "# !pip install torch==1.7.0+cu101 torchvision==0.8.1+cu101 torchaudio==0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "\n",
        "# For interactive demo at the end:\n",
        "!pip install pydub\n",
        "!pip install torchaudio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchaudio\n",
        "import math\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSiyBLsmrvde",
        "outputId": "be34fc5d-833a-49b0-f553-b1dceb03c8e4"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98gvaRdwU2ML"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwm4uTOMqbcI"
      },
      "source": [
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_len, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # add pe constatns to embeddings\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        # print(f\"PE Shape:{self.pe.shape}\")\n",
        "        # print(f\"X shape: {x.shape}\")\n",
        "        # print(f\"Seq Len: {seq_len}\")\n",
        "        # print(f\"Added PE Shape: {self.pe[:,:seq_len,:].shape}\")\n",
        "\n",
        "        x = x + self.pe[:,:seq_len, :]  #.cuda().detach()\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, N_encoder_layers, heads, d_ff, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.N = N_encoder_layers\n",
        "        # self.embed = Embedder(vocab_size, d_model)\n",
        "        self.pe = PositionalEncoder(d_model, max_seq_len)\n",
        "        self.encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, heads, d_ff, dropout=0.3),\n",
        "            self.N\n",
        "        )\n",
        "        # self.layers = get_clones(\n",
        "        #     EncoderLayer(d_model, heads, d_ff),\n",
        "        #     # nn.TransformerEncoderLayer(d_model, heads, d_ff),\n",
        "        #      self.N)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "    def forward(self, src):\n",
        "        # print(f\"Before PE X shape: {src.shape}\")\n",
        "        x = self.pe(src)\n",
        "\n",
        "        # x = (batch_size, seq_len, d_model)\n",
        "        # but Transformer Encoder layer accepts\n",
        "        # (seq_len, batch_size, d_model)\n",
        "        x = x.transpose(0,1)\n",
        "        x = self.encoder(x)\n",
        "        # for i in range(self.N):\n",
        "        #     x = self.layers[i](x)\n",
        "        x = x.transpose(0,1)\n",
        "        \n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time) \n",
        "\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    \"\"\"Residual CNN inspired by https://arxiv.org/pdf/1603.05027.pdf\n",
        "        except with layer norm instead of batcorch.Size([1, 43h norm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        # print(f\"Before Layer Norm shape: {x.shape}\")\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "class SpeechModel(nn.Module):\n",
        "    def __init__(self, max_seq_len, n_feats, N_cnn_layers, n_channels,\\\n",
        "                 N_encoder_layers, d_model, d_ff, heads,N_AUs,\\\n",
        "                 stride=2, dropout=0.1,\\\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Conv2d(1, n_channels, 3, stride=1, padding=3//2)\n",
        "        # cnn for extracting heirachal features\n",
        "\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(n_channels, n_channels, kernel=3, stride=1, dropout=dropout, n_feats=n_feats) \n",
        "            for _ in range(N_cnn_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(n_channels*n_feats, d_model) #n_channels\n",
        "\n",
        "        self.encoder = Encoder(d_model, N_encoder_layers, heads, d_ff, max_seq_len)\n",
        "        self.linear = nn.Linear(d_model, N_AUs)\n",
        "\n",
        "    def forward(self, src):  # input: (batch, 1, feature, time)\n",
        "        x = self.cnn(src)    # (batch, channel, feature, time)\n",
        "        # print(f\"After CNN shape: {x}\")\n",
        "\n",
        "        x = self.rescnn_layers(x)\n",
        "        # print(f\"After RES shape: {x}\")\n",
        "\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1]*sizes[2], sizes[3])  # (batch, channel*feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, channel*feature)\n",
        "        x = self.fc(x)        # (batch, time, d_model)\n",
        "\n",
        "        # print(f\"Befor Encoder X shape: {x}\")\n",
        "\n",
        "        e_outputs = self.encoder(x) # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # print(f\"Encoder Output: {e_outputs}\")\n",
        "        avg_enc   = torch.mean(x, -2)   # (batch_size, d_model)\n",
        "\n",
        "        # print(f\"Average Encoder: {avg_enc}\")\n",
        "        output = self.linear(avg_enc)  #(batch_size, N_AUs)\n",
        "\n",
        "        # print(f\"Output before Sigmoid: {output}\")\n",
        "        output = 5*nn.Sigmoid()(output)\n",
        "        # print(f\"Output : {output}\")\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nLSPfrhqayp",
        "outputId": "d79b624e-bd03-46a8-b011-3b075c0a31d9"
      },
      "source": [
        "heads = 8\n",
        "N_encoder_layers = 4\n",
        "d_model = 512 #512\n",
        "d_ff = 2048 #2048\n",
        "N_AUs = 17  # TO BE EDIT\n",
        "\n",
        "max_seq_len = 441 #max_seq_length # TO BE EDIT\n",
        "n_channels = 16\n",
        "N_cnn_layers = 1\n",
        "n_feats = 128 # TO BE EDIT\n",
        "n_mels = 128\n",
        "\n",
        "model = SpeechModel(max_seq_len, n_feats, N_cnn_layers, n_channels, N_encoder_layers,\\\n",
        "                    d_model, d_ff, heads, N_AUs)\n",
        "\n",
        "data_dir = '/content/drive/MyDrive/ASR_Project/'\n",
        "model.load_state_dict(torch.load(data_dir+'SunilModel', map_location=torch.device('cpu')))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ofrr2czRTx7S"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyqWJMOJuDGq",
        "outputId": "0769c871-ae6f-4d4e-e253-ea0d8b148575"
      },
      "source": [
        "import pandas as pd\n",
        "resample_transform = torchaudio.transforms.Resample(44100, 16000)\n",
        "melspec_transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000)\n",
        "\n",
        "\n",
        "def get_audio(file_path):\n",
        "    waveform, _ = torchaudio.load(file_path)\n",
        "    waveform = resample_transform(waveform)\n",
        "    return waveform\n",
        "\n",
        "def get_au(file_path):\n",
        "    df = pd.read_csv(file_path, delimiter=',')\n",
        "    columns = df.columns[1:]\n",
        "    au_data = torch.tensor(df.mean().values[1:])\n",
        "    return au_data, columns\n",
        "\n",
        "\n",
        "def get_output(model, wave):\n",
        "    model.eval()\n",
        "    wave_melspec = melspec_transform(wave)\n",
        "    wave_data    = wave_melspec.mean(dim=0, keepdims=True).unsqueeze(0)\n",
        "    model_output = model(wave_data)[0]\n",
        "    return model_output\n",
        "\n",
        "\n",
        "def inference(model, audio_file_path, au_file_path):\n",
        "    waveform = get_audio(audio_file_path)\n",
        "    target_aus, au_header = get_au(au_file_path)\n",
        "\n",
        "    generated_aus = get_output(model, waveform)\n",
        "\n",
        "    # loss = nn.\n",
        "    return generated_aus, target_aus, au_header"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchaudio/functional/functional.py:358: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  \"At least one mel filterbank has all zero values. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvG7fU6MuPGj"
      },
      "source": [
        "au_dir = data_dir+'au_dir/'\n",
        "audio_dir = data_dir+'split_audios_dir/'\n",
        "\n",
        "\n",
        "my_audio_file_path = '/content/7qgWBZu9VOc-008.mp3'\n",
        "my_au_file_path = '/content/7qgWBZu9VOc-008.mp4.csv'\n",
        "\n",
        "generated_au, target_au,au_header = inference(model, my_audio_file_path, my_au_file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-C6be09vQa_",
        "outputId": "c6f85b51-4267-4d3e-93ed-5da49f6fd125"
      },
      "source": [
        "print(f\"{au_header.values}\")\n",
        "print(f\"Generated\\n{generated_au.detach().numpy()}\")\n",
        "print(f\"Target\\n{target_au.detach().numpy()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['AU01_r' 'AU02_r' 'AU04_r' 'AU05_r' 'AU06_r' 'AU07_r' 'AU09_r' 'AU10_r'\n",
            " 'AU12_r' 'AU14_r' 'AU15_r' 'AU17_r' 'AU20_r' 'AU23_r' 'AU25_r' 'AU26_r'\n",
            " 'AU45_r']\n",
            "Generated\n",
            "[0.7896003  0.6537746  1.1176057  0.64682376 0.9194159  1.3153814\n",
            " 0.66935724 1.1891489  0.919821   1.0739055  0.73496497 0.98586625\n",
            " 0.6794081  0.6868584  1.1004884  1.0184083  0.78645414]\n",
            "Target\n",
            "[0.5356     0.11653333 2.35333333 0.03853333 1.07193333 2.18373333\n",
            " 0.02673333 0.82606667 0.48886667 1.89146667 0.10206667 0.21033333\n",
            " 0.06093333 0.04686667 0.39793333 0.449      0.35826667]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNi538TPv5Lv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}